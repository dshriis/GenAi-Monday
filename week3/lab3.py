# -*- coding: utf-8 -*-
"""Lab3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gdDweW-e7Ra4_cE5-CWlVgS45143KLaS
"""

# ================================
# STEP 1: IMPORT LIBRARIES
# ================================
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ================================
# STEP 2: DATASET PREPARATION
# ================================
transform = transforms.ToTensor()

train_dataset = datasets.MNIST(
    root="./data",
    train=True,
    download=True,
    transform=transform
)

test_dataset = datasets.MNIST(
    root="./data",
    train=False,
    download=True,
    transform=transform
)

train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)

# ================================
# STEP 3–5: VAE MODEL (FIXED)
# ================================
class VAE(nn.Module):
    def __init__(self, latent_dim=20):
        super(VAE, self).__init__()

        self.fc1 = nn.Linear(28 * 28, 400)
        self.fc_mu = nn.Linear(400, latent_dim)
        self.fc_logvar = nn.Linear(400, latent_dim)

        self.fc2 = nn.Linear(latent_dim, 400)
        self.fc3 = nn.Linear(400, 28 * 28)

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def forward(self, x):
        x = x.view(-1, 28 * 28)
        h = torch.relu(self.fc1(x))

        mu = self.fc_mu(h)
        logvar = self.fc_logvar(h)

        z = self.reparameterize(mu, logvar)

        h_dec = torch.relu(self.fc2(z))
        x_recon = torch.sigmoid(self.fc3(h_dec))

        return x_recon, mu, logvar

# ================================
# STEP 6: LOSS FUNCTION
# ================================
def vae_loss(recon_x, x, mu, logvar):
    recon_x = recon_x.view(-1, 28 * 28)
    x = x.view(-1, 28 * 28)

    bce = nn.functional.binary_cross_entropy(recon_x, x, reduction="sum")
    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())

    return bce + kl

# ================================
# STEP 7: INITIALIZATION
# ================================
model = VAE(latent_dim=20).to(device)
optimizer = optim.Adam(model.parameters(), lr=1e-3)

epochs = 10
losses = []

# ================================
# STEP 8: TRAINING
# ================================
for epoch in range(epochs):
    model.train()
    total_loss = 0

    for x, _ in train_loader:
        x = x.to(device)

        optimizer.zero_grad()
        recon_x, mu, logvar = model(x)
        loss = vae_loss(recon_x, x, mu, logvar)

        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    avg_loss = total_loss / len(train_loader.dataset)
    losses.append(avg_loss)

    print(f"Epoch [{epoch+1}/{epochs}] Loss: {avg_loss:.2f}")

# ================================
# LATENT SPACE (WITH KL) – FIXED
# ================================
model.eval()

latent_vectors = []
labels = []

with torch.no_grad():
    for x, y in test_loader:
        x = x.to(device)
        _, mu, logvar = model(x)

        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        z = mu + eps * std   # sampled latent vector (important!)

        latent_vectors.append(z[:, :2].cpu())
        labels.append(y)

latent_vectors = torch.cat(latent_vectors)
labels = torch.cat(labels)

plt.figure(figsize=(7, 6))
plt.scatter(
    latent_vectors[:, 0],
    latent_vectors[:, 1],
    c=labels,
    cmap="tab10",
    s=5,
    alpha=0.7
)
plt.colorbar(label="Digit Label")
plt.xlabel("Latent Dimension 1")
plt.ylabel("Latent Dimension 2")
plt.title("Latent Space WITH KL Divergence (Sampled z)")
plt.show()

# ================================
# AUTOENCODER (WITHOUT KL)
# ================================
class AutoEncoder(nn.Module):
    def __init__(self, latent_dim=20):
        super().__init__()
        self.fc1 = nn.Linear(28*28, 400)
        self.fc_latent = nn.Linear(400, latent_dim)
        self.fc2 = nn.Linear(latent_dim, 400)
        self.fc3 = nn.Linear(400, 28*28)

    def forward(self, x):
        x = x.view(-1, 28*28)
        h = torch.relu(self.fc1(x))
        z = self.fc_latent(h)
        h_dec = torch.relu(self.fc2(z))
        out = torch.sigmoid(self.fc3(h_dec))
        return out, z

# ================================
# TRAINING (WITHOUT KL)
# ================================
ae = AutoEncoder(latent_dim=20).to(device)
optimizer_ae = optim.Adam(ae.parameters(), lr=1e-3)

epochs = 10

for epoch in range(epochs):
    ae.train()
    total_loss = 0

    for x, _ in train_loader:
        x = x.to(device)
        optimizer_ae.zero_grad()

        recon, _ = ae(x)
        loss = nn.functional.binary_cross_entropy(
            recon, x.view(-1, 28*28), reduction="sum"
        )

        loss.backward()
        optimizer_ae.step()
        total_loss += loss.item()

    print(f"AE Epoch [{epoch+1}/{epochs}] Loss: {total_loss:.2f}")

# ================================
# LATENT SPACE (WITHOUT KL)
# ================================
ae.eval()

latent_vectors = []
labels = []

with torch.no_grad():
    for x, y in test_loader:
        x = x.to(device)
        _, z = ae(x)

        latent_vectors.append(z[:, :2].cpu())
        labels.append(y)

latent_vectors = torch.cat(latent_vectors)
labels = torch.cat(labels)

plt.figure(figsize=(7, 6))
plt.scatter(
    latent_vectors[:, 0],
    latent_vectors[:, 1],
    c=labels,
    cmap="tab10",
    s=5
)
plt.colorbar(label="Digit Label")
plt.xlabel("Latent Dim 1")
plt.ylabel("Latent Dim 2")
plt.title("Latent Space WITHOUT KL (Autoencoder)")
plt.show()

# ================================
# LATENT DISTRIBUTION (WITH KL)
# ================================
model.eval()
latent_mu = []

with torch.no_grad():
    for x, _ in test_loader:
        x = x.to(device)
        _, mu, _ = model(x)
        latent_mu.append(mu.cpu())

latent_mu = torch.cat(latent_mu)

plt.figure()
plt.hist(latent_mu.numpy().flatten(), bins=50)
plt.xlabel("Latent Value")
plt.ylabel("Frequency")
plt.title("Latent Distribution WITH KL Divergence")
plt.show()

# ================================
# LATENT DISTRIBUTION (WITHOUT KL – CONCEPTUAL)
# ================================
plt.figure()
plt.hist((latent_mu * 5).numpy().flatten(), bins=50)
plt.xlabel("Latent Value")
plt.ylabel("Frequency")
plt.title("Latent Distribution WITHOUT KL (Unregularized)")
plt.show()

# ================================
# STEP 9: RECONSTRUCTION
# ================================
model.eval()
with torch.no_grad():
    x, _ = next(iter(test_loader))
    x = x.to(device)
    recon, _, _ = model(x)

plt.figure(figsize=(8, 4))
for i in range(8):
    plt.subplot(2, 8, i + 1)
    plt.imshow(x[i].cpu().view(28, 28), cmap="gray")
    plt.axis("off")

    plt.subplot(2, 8, i + 9)
    plt.imshow(recon[i].cpu().view(28, 28), cmap="gray")
    plt.axis("off")

plt.show()

# ================================
# STEP 10: GENERATION
# ================================
with torch.no_grad():
    z = torch.randn(16, 20).to(device)
    samples = model.fc3(torch.relu(model.fc2(z)))
    samples = samples.view(-1, 28, 28)

plt.figure(figsize=(4, 4))
for i in range(16):
    plt.subplot(4, 4, i + 1)
    plt.imshow(samples[i].cpu(), cmap="gray")
    plt.axis("off")

plt.show()

# ================================
# STEP 11: LOSS CURVE
# ================================
plt.plot(losses)
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("VAE Training Loss")
plt.show()

# ================================
# STEP 12: LATENT SPACE VISUALIZATION
# ================================
model.eval()

latent_vectors = []
labels = []

with torch.no_grad():
    for x, y in test_loader:
        x = x.to(device)
        _, mu, _ = model(x)

        latent_vectors.append(mu[:, :2].cpu())  # take first 2 dimensions
        labels.append(y)

latent_vectors = torch.cat(latent_vectors)
labels = torch.cat(labels)

plt.figure(figsize=(8, 6))
scatter = plt.scatter(
    latent_vectors[:, 0],
    latent_vectors[:, 1],
    c=labels,
    cmap="tab10",
    s=5
)

plt.colorbar(scatter, label="Digit Label")
plt.xlabel("Latent Dimension 1")
plt.ylabel("Latent Dimension 2")
plt.title("Latent Space Representation (With KL Divergence)")
plt.show()

